{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# %%\n",
    "# more current chroma database\n",
    "if os.path.exists('./chroma'):\n",
    "    shutil.rmtree('./chroma')\n",
    "\n",
    "# %%\n",
    "# with open('secret/openai_api_key.txt') as f:\n",
    "#     openai_api_key = f.read()\n",
    "\n",
    "# %%\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# embedding_llm = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "embedding_llm = OllamaEmbeddings(model=\"llama3.2\")\n",
    "\n",
    "# %%\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# %% [markdown]\n",
    "# # Files & Loading\n",
    "\n",
    "# %%\n",
    "from pathlib import Path\n",
    "\n",
    "# %%\n",
    "docs_base = Path(\"./docs\")\n",
    "files_all = docs_base.rglob(\"*\")\n",
    "excludes = [\".DS_Store\", \".ipynb_checkpoints\"]\n",
    "files = []\n",
    "for f in files_all:\n",
    "    to_exclude = False\n",
    "    for ex in excludes:\n",
    "        to_exclude = to_exclude or f.match(ex)\n",
    "    if not to_exclude and f.is_file():\n",
    "        files.append(f)\n",
    "\n",
    "# %%\n",
    "files\n",
    "\n",
    "# %%\n",
    "documents = []\n",
    "for file in files:\n",
    "    print(file)\n",
    "    loader = TextLoader(str(file.absolute()),encoding='utf-8')\n",
    "    documents += loader.load()\n",
    "\n",
    "# %%\n",
    "documents\n",
    "\n",
    "# %%\n",
    "documents[0].__dict__\n",
    "\n",
    "# %% [markdown]\n",
    "# # Indexing with Chroma\n",
    "\n",
    "# %%\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# %%\n",
    "db = Chroma.from_documents(documents, embedding_llm, persist_directory=\"./chroma\",\n",
    "                          collection_name=\"planetbucks\")\n",
    "\n",
    "# %%\n",
    "db.persist()\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.prompts import SystemMessagePromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate, PromptTemplate\n",
    "\n",
    "# %% [markdown]\n",
    "# # Create Embedding LLM (OpenAI)\n",
    "\n",
    "# %%\n",
    "# with open('./openai_api_key.txt') as f:\n",
    "#     openai_api_key = f.read()\n",
    "# embedding_llm = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "embedding_llm = OllamaEmbeddings(model=\"llama3.2\")\n",
    "\n",
    "# %% [markdown]\n",
    "# # Load Databases\n",
    "\n",
    "# %%\n",
    "db = Chroma(persist_directory='./chroma', embedding_function=embedding_llm,\n",
    "            collection_name=\"planetbucks\")\n",
    "\n",
    "# %% [markdown]\n",
    "# # Using Chroma vector database\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Similarity search\n",
    "\n",
    "# %%\n",
    "query = \"opening hours\"\n",
    "result = db.similarity_search_with_relevance_scores(query)\n",
    "# result = db.similarity_search_with_score(query)\n",
    "print(len(result))\n",
    "\n",
    "# %%\n",
    "result\n",
    "\n",
    "# %%\n",
    "result = db.similarity_search_with_relevance_scores(query, k=2)\n",
    "print(len(result))\n",
    "\n",
    "# %%\n",
    "result\n",
    "\n",
    "# %% [markdown]\n",
    "# ## MMR (Maximum marginal relevance)\n",
    "# - MMR = iteratively find documents that are dissimilar to previous results.\n",
    "# - It could improve performance for retrievals.\n",
    "\n",
    "# %%\n",
    "db.max_marginal_relevance_search(query, k=2)\n",
    "\n",
    "# %% [markdown]\n",
    "# # Retriever\n",
    "\n",
    "# %%\n",
    "retriever = db.as_retriever(search_type='mmr', search_kwargs={'k': 2, 'fetch_k': 5})\n",
    "\n",
    "# %%\n",
    "retriever.invoke(query)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.prompts import SystemMessagePromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate, PromptTemplate\n",
    "\n",
    "# %% [markdown]\n",
    "# # Create Embedding LLM (OpenAI)\n",
    "\n",
    "# %%\n",
    "# with open('./openai_api_key.txt') as f:\n",
    "#     openai_api_key = f.read()\n",
    "# embedding_llm = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "embedding_llm = OllamaEmbeddings(model=\"llama3.2\")\n",
    "\n",
    "# %% [markdown]\n",
    "# # Load Databases\n",
    "\n",
    "# %%\n",
    "db = Chroma(persist_directory='./chroma', embedding_function=embedding_llm,\n",
    "            collection_name=\"planetbucks\")\n",
    "\n",
    "# %% [markdown]\n",
    "# # Creating RAG tools\n",
    "\n",
    "# %%\n",
    "retriever_tool = create_retriever_tool(\n",
    "    db.as_retriever(search_type='mmr'),\n",
    "    name = \"planetbucks_search\",\n",
    "    description = \"\"\"Search for information about PlanetBucks store, including store information, \n",
    "    coffee drink menus, specialty coffee beans menu, and bean fact sheet.\"\"\",\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# # (Legacy) Create AgentExecutor\n",
    "\n",
    "# %%\n",
    "# # from langchain_openai import ChatOpenAI\n",
    "# # from langchain_ollama import OllamaLLM as Ollama\n",
    "# from langchain_ollama.chat_models import ChatOllama\n",
    "# from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# # llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, openai_api_key=openai_api_key)\n",
    "# llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "\n",
    "# %%\n",
    "# system_message = \"\"\"You are a helpful assistant working at a coffee shop.\n",
    "# You can use a given chat history and given tools to respond to a user.\n",
    "# Your character is a polite and friendly female.\n",
    "# You answer concisely, without introduction or appending.\n",
    "# If you do not know the answer, just say 'I do not know.'\n",
    "# When you calculate the price of an ordered item, you should think step-by-step.\n",
    "# \"\"\"\n",
    "\n",
    "# %%\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=system_message)),\n",
    "#         MessagesPlaceholder(\n",
    "#             variable_name=\"chat_history\", optional=True\n",
    "#         ),  # Where the memory will be stored.\n",
    "#         HumanMessagePromptTemplate.from_template(\n",
    "#             \"{input}\"\n",
    "#         ),  # Where the human input will injected\n",
    "#         MessagesPlaceholder(\n",
    "#             variable_name=\"agent_scratchpad\"\n",
    "#         ),  # Where the memory will be stored.\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# %%\n",
    "# tools = [retriever_tool, ]\n",
    "\n",
    "# %%\n",
    "# from langchain.agents import AgentExecutor, create_openai_tools_agent, create_openai_functions_agent\n",
    "# from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# agent = create_openai_functions_agent(llm, tools, prompt,)\n",
    "\n",
    "# memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, memory=memory)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ChatInterface\n",
    "\n",
    "# %%\n",
    "# def wrapper_chat_history(chat_history, memory):\n",
    "#     chat_history = []\n",
    "#     for m in memory.chat_memory.messages:\n",
    "#         chat_history.append(m.content)\n",
    "#     return chat_history\n",
    "\n",
    "# %%\n",
    "# def converse(message, chat_history):\n",
    "#     response = agent_executor.invoke({\"input\": message})\n",
    "#     chat_history = wrapper_chat_history(chat_history, memory)\n",
    "#     return response['output']\n",
    "\n",
    "# %%\n",
    "# import gradio as gr\n",
    "\n",
    "# demo = gr.ChatInterface(fn=converse)\n",
    "\n",
    "# demo.launch(share=False)\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# # Agent with LangGraph\n",
    "\n",
    "# %%\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# %%\n",
    "llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "tools = [retriever_tool, ]\n",
    "\n",
    "SYSTEM_MESSAGE = \"\"\"You are a helpful receptionist at PlanetBucks, which is a coffee cafe. Your name i Echo.\n",
    "You wil answer politely but playfully since it's Christmas festival time now.\n",
    "\"\"\"\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "langgraph_agent_executor = create_react_agent(\n",
    "    llm,\n",
    "    tools,\n",
    "    state_modifier=SYSTEM_MESSAGE,\n",
    "    checkpointer=memory\n",
    ")\n",
    "\n",
    "# %%\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        langgraph_agent_executor.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# %%\n",
    "config = {\"configurable\": {\"thread_id\": \"thread1\"}}\n",
    "\n",
    "query = \"I'm Ismail. Hello.\"\n",
    "\n",
    "response = langgraph_agent_executor.invoke(\n",
    "    {\"messages\": [(\"human\", query)]},\n",
    "    config\n",
    ")\n",
    "\n",
    "# %%\n",
    "response\n",
    "\n",
    "# %%\n",
    "query = \"What is opening hour?\"\n",
    "\n",
    "response = langgraph_agent_executor.invoke({\"messages\": [(\"human\", query)]}, config)\n",
    "\n",
    "# %%\n",
    "response\n",
    "\n",
    "# %%\n",
    "len(response['messages'])\n",
    "\n",
    "# %%\n",
    "response['messages']\n",
    "\n",
    "# %%\n",
    "query = \"What's your name?\"\n",
    "\n",
    "response = langgraph_agent_executor.invoke({\"messages\": [(\"human\", query)]}, config)\n",
    "\n",
    "# %%\n",
    "response\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ChatInterface\n",
    "\n",
    "# %%\n",
    "import gradio as gr\n",
    "\n",
    "# %%\n",
    "config = {\"configurable\": {\"thread_id\": \"thread1\"}}\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "tools = [retriever_tool, ]\n",
    "\n",
    "\n",
    "SYSTEM_MESSAGE = \"\"\"You are a helpful receptionist at PlanetBucks, which is a coffee cafe. Your name i Echo.\n",
    "You wil answer politely but playfully since it's Christmas festival time now.\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "try:\n",
    "    demo.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "global chat_history\n",
    "chat_history = []\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "langgraph_agent_executor = create_react_agent(\n",
    "    llm,\n",
    "    tools,\n",
    "    state_modifier=SYSTEM_MESSAGE,\n",
    "    checkpointer=memory\n",
    ")\n",
    "\n",
    "def chat(message, history):\n",
    "    global chat_history\n",
    "    response = langgraph_agent_executor.invoke(\n",
    "        {\"messages\": [(\"human\", message)]},\n",
    "        config\n",
    "        )\n",
    "    chat_history = response\n",
    "    return response['messages'][-1].content\n",
    "\n",
    "demo = gr.ChatInterface(fn=chat)\n",
    "\n",
    "demo.launch(share=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "chat_history\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
