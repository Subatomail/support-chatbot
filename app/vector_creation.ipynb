{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_chroma_directory(directory: str) -> None:\n",
    "    \"\"\"Remove the existing Chroma directory if it exists.\"\"\"\n",
    "    if os.path.exists(directory):\n",
    "        shutil.rmtree(directory)\n",
    "        print(f\"Removed existing Chroma directory: {directory}\")\n",
    "    else:\n",
    "        print(f\"No existing Chroma directory found at: {directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_model(model_name: str = \"llama3.2\"):\n",
    "    \"\"\"Get an embedding model. Default: Ollama Embeddings.\"\"\"\n",
    "    print(f\"Loading embedding model: {model_name}\")\n",
    "    return OllamaEmbeddings(model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(docs_path: str, exclude_patterns=None) -> list:\n",
    "    if exclude_patterns is None:\n",
    "        exclude_patterns = [\".DS_Store\", \".ipynb_checkpoints\"]\n",
    "\n",
    "    base_path = Path(docs_path)\n",
    "    all_files = base_path.rglob(\"*\")\n",
    "\n",
    "    documents = []\n",
    "    for file in all_files:\n",
    "        if not file.is_file():\n",
    "            continue\n",
    "\n",
    "        if any(file.match(pattern) for pattern in exclude_patterns):\n",
    "            continue\n",
    "\n",
    "        # Decide loader by extension\n",
    "        if file.suffix.lower() == \".pdf\":\n",
    "            print(f\"Loading PDF: {file}\")\n",
    "            loader = PyPDFLoader(str(file.absolute()))\n",
    "        else:\n",
    "            print(f\"Loading text: {file}\")\n",
    "            # Fallback to TextLoader\n",
    "            loader = TextLoader(str(file.absolute()), encoding='utf-8')\n",
    "\n",
    "        documents.extend(loader.load())\n",
    "\n",
    "    print(f\"Total documents loaded: {len(documents)}\")\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_documents(documents: list, chunk_size: int = 1000, chunk_overlap: int = 200) -> list:\n",
    "    \"\"\"\n",
    "    Splits documents into smaller chunks for better retrieval.\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "\n",
    "    chunked_docs = []\n",
    "    for doc in documents:\n",
    "        # split_documents() expects a list of Documents.\n",
    "        doc_chunks = splitter.split_documents([doc])\n",
    "        chunked_docs.extend(doc_chunks)\n",
    "\n",
    "    print(f\"Total documents after chunking: {len(chunked_docs)}\")\n",
    "    return chunked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vectorstore(docs: list, embeddings, persist_dir: str, collection: str):\n",
    "    \"\"\"Build and persist a Chroma vectorstore from documents.\"\"\"\n",
    "    db = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_dir,\n",
    "        collection_name=collection\n",
    "    )\n",
    "    print(f\"Chroma DB persisted at: {persist_dir}\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing Chroma directory found at: ./chroma\n",
      "Loading embedding model: llama3.2\n",
      "Loading PDF: data\\products\\bpcms-1500-pro-series-product-sheet-ver-mac-003.pdf\n",
      "Loading text: data\\products\\pcms.txt\n",
      "Total documents loaded: 3\n",
      "Total documents after chunking: 11\n",
      "Chroma DB persisted at: ./chroma\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 1) Remove old Chroma data\n",
    "    remove_chroma_directory('./chroma')\n",
    "\n",
    "    # 2) Get embedding model\n",
    "    embedding_llm = get_embedding_model(\"llama3.2\")\n",
    "\n",
    "    # 3) Load documents\n",
    "    raw_documents = load_documents(\"./data\")\n",
    "\n",
    "    # 4) Chunk documents\n",
    "    chunked_documents = chunk_documents(raw_documents, chunk_size=512, chunk_overlap=128)\n",
    "\n",
    "    # 5) Build and persist vectorstore\n",
    "    db = build_vectorstore(\n",
    "        docs=chunked_documents,\n",
    "        embeddings=embedding_llm,\n",
    "        persist_dir=\"./chroma\",\n",
    "        collection=\"vermac-support\"\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
